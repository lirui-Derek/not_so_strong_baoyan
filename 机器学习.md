# 1、特征选择的三种方法

​		降维的过程。

1、减少特征数量、降维，使模型泛化能力更强，减少过拟合
2、增强对特征和特征值之间的理解

在[特征选择](https://so.csdn.net/so/search?q=特征选择&spm=1001.2101.3001.7020)中涉及到两个过程，一个是**子集搜索**，一个是**子集评价**。已知的特征空间的[维度](https://so.csdn.net/so/search?q=维度&spm=1001.2101.3001.7020)，需要去遍历多有可能的子集显然不现实。所以一个可行的做法是，先产生一个候选的子集，然后对该子集进行评价，之后根据这个评价继续搜索特征。

1、过滤式

过滤式方法指的是先对特征集进行筛选，然后再进行学习器的训练，特征选择过程对后续的学习器无关。相当于先用特征选择的过程对初始的特征进行过滤，再用过滤后的特征进行模型的训练。

2、包裹式

包裹式的特征选择考虑到具体的学习器，是根据学习器上的误差来评价特征子集的优劣，在子集的搜索方式上是用了拉维加斯的随机策略。

3、嵌入式

嵌入式的方法与之前的两个方法不同，嵌入式将特征选择的过程与学习器的训练过程融为一体，在学习器的训练过程中，自动的进行了特征的选择



# 2、逻辑回归和线性回归的区别

1）线性回归要求变量服从正态分布，logistic回归对变量分布没有要求。
2）线性回归要求因变量是连续性数值变量，而logistic回归要求因变量是分类型变量。
3）线性回归要求自变量和因变量呈线性关系，而logistic回归不要求自变量和因变量呈线性关系。
4）logistic回归是分析因变量取某个值的概率与自变量的关系，而线性回归是直接分析因变量与自变量的关系。



# 3、范数

## 1、向量的范数

0-范数：向量中非0元素的个数；

1-范数：绝对值之和；

2-范数：$||X||_2 = (\sum_{i=1}^{N}|x_i|^2)^{\frac{1}{2}}$; 元素的绝对值的平方和的开方

Lp范数: 为x向量各个元素绝对值p次方和的1/p次方。



## 2、矩阵的范数

1-范数：$||A||_1 = max(\sum{|a_1|,|a_2|...})$ 列绝对值和的最大值

2-范数：$||A||_2 = A的最大奇异值 = $





# 4、SVM

​		支持向量机学习方法包括构建由简至繁的模型：**线性可分支持向量机、线性支持向量机及非线性支持向量机**。当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

​		核心是找到区分两类训练样本的超平面。

![这里写图片描述](https://img-blog.csdn.net/20170211150553631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzU1MTIyNDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​		在样本空间，超平面用如下方程来描述：
$$
w^Tx + b = 0
$$
​		一个点距离超平面的距离d的大小可以表示分类预测的确信程度。在超平面确定的情况下，
$$
d = \frac{|w^Tx + b|}{||w||}
$$
​		当点A表示某一实例，其类标记为+，点A与超平面的距离记作d_i，则
$$
d_i = \frac{w^Tx + b}{||w||}
$$
​		当点A表示某一实例，其类标记为-，点A与超平面的距离记作d_i，则
$$
d_i = -\frac{w^Tx + b}{||w||}
$$
​		一般地，点与超平面的距离是
$$
d_i = y_i\frac{w^Tx + b}{||w||}
$$
​		也被称为超平面关于样本点的几何间隔。

### 最大间隔分离超平面

![这里写图片描述](https://img-blog.csdn.net/20170211153326794?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzU1MTIyNDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​		间隔
$$
d = \frac{2}{||w||}
$$
​		要求最大化间隔，就是要满足：
$$
max \frac{2}{||w||}   s.t. y_i(w^Tx_i + b) \geqslant 1
$$


​		可重写为
$$
min \frac{1}{2}||w||^2  s.t. y_i(w^Tx_i + b) \geqslant 1
$$

# 5、极大似然估计

极大似然估计(Maximum likelihood estimation, 简称MLE)是统计学中常用的参数估计方法，极大似然估计的关键就是，利用已知的样本结果信息，反推最大概率导致这些样本结果出现的模型参数值。

也就是首先假定其具有某种确定的概率分布，但是其参数未知，然后基于训练样本对概率分布的参数进行估计。

**我们所估计的模型参数，要使得产生这个给定样本的可能性最大**。
