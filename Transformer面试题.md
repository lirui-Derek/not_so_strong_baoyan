Transformer面试题

![史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！](https://pic1.zhimg.com/v2-af0c915ed31ea449349c2948940fc5df_1440w.jpg?source=172ae18b)

#### 1、Transformer为何使用多头注意力机制？（为什么不使用一个头）

答：多头可以使参数矩阵形成多个子空间，矩阵整体的size不变，只是改变了每个head对应的维度大小，这样做使矩阵对多方面信息进行学习，但是计算量和单个head差不多。



#### 2、Transformer如何并行化的？

答：Transformer的并行化我认为主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，在self-attention模块，对于某个序列，self-attention模块可以直接计算$x_{i}$,$x_{j}$的点乘结果，而RNN系列的模型就必须按照顺序从头计算到尾。



#### 3、Transformers输入是并行的，与输出的串行输出是如何对应的

encoder的输出并没直接作为decoder的直接输入。

训练的时候，1.初始decoder的time step为1时(也就是第一次接收输入)，其输入为一个特殊的token，可能是目标序列开始的token(如)，也可能是源序列结尾的token(如)，也可能是其它视任务而定的输入等等，不同源码中可能有微小的差异，其目标则是预测翻译后的第1个单词(token)是什么；2.然后和预测出来的第1个单词一起，再次作为decoder的输入，得到第2个预测单词；3后续依此类推；



#### 4、残差网络为什么能解决梯度消失问题。

[残差网络]（Residual Network）是一种非常有效的缓解梯度消失问题网络，极大的提高了可以有效训练的网络的深度。残差单元可以以**跳层连接**的形式实现，即将单元的输入直接与单元输出加在一起，然后再激活。不会出现网络退化的问题。



#### 5、layernorm和batchnorm

BatchNorm是对一个batch-size样本内的每个特征做归一化，LayerNorm是对每个样本的所有特征做归一化。

形象点来说，假设有一个二维矩阵。行为batch-size，列为样本特征。那么BN就是竖着归一化，LN就是横着归一化。

它们的出发点都是让该层参数稳定下来，避免梯度消失或者梯度爆炸，方便后续的学习。但是也有侧重点。

Layernorm能够比较好地适应NLP文本长度不一致的情况。



#### 6、注意力机制与自注意力机制

注意力机制就是对输入权重分配的关注，最开始使用到注意力机制是在`编码器-解码器`(encoder-decoder)中, 注意力机制通过对编码器所有时间步的隐藏状态做加权平均来得到下一层的输入变量。



注意力机制：主要是引入了注意力机制这个概念，比较有代表性的是SENet。通过对每一个特征层进行全局池化，再到全连接层中找特征之间的联系，最后得到权重划分。

自注意力机制：主要是根据两两之间的关系来引入权重，在通道、空间两个层面，通过计算每个单元通道与通道之间、像素点与像素点之间的值，来加强两两之间的联系，进而提高精确度语义分割。



#### 7、Transformer的优缺点

优点：

1、每层的计算复杂度小。

2、可并行化的计算。

3、可以直接计算每个词之间的相关性，不需要通过隐藏层传递。

4、自注意力模型更可解释。

缺点：

**1. 局部信息的获取不如RNN和CNN强**

**2. 位置信息编码存在问题**

**3. 顶层梯度消失**



