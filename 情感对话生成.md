# 情感对话生成

## 1、post embedding

![在这里插入图片描述](https://img-blog.csdnimg.cn/52203e19772840bb9503664d3c350cc1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSn6Jm-6aOe5ZOl5ZOl,size_20,color_FFFFFF,t_70,g_se,x_16)

​			GRU——门控循环单元结构，它也是传统RNN的变体，能够有效捕捉长序列之间的语义关联，缓解梯度消失或爆炸现象。

核心结构：

- 更新门：能够关注的机制
  $$
  R_t = \sigma(X_tW_{xr} + H_{t-1}W_{hr} + b_r)
  $$
  
- 重置门：能够遗忘的机制
  $$
  Z_t = \sigma(X_tW_{xz} + H_{t-1}W_{hz}+b_r)
  $$

- GRU候选隐状态
  $$
  \widetilde{H_t} = \sigma(X_tW_{xh} + (R_t \bigodot H_{t-1})W_{hh} + b_h)
  $$

- 隐藏状态
  $$
  H_t = Z_t \bigodot H_{t-1} + (1-Z_t) \bigodot \widetilde{H_t}
  $$

​			

## 1、关键词生成

情感关键词生成——

## 2、主题关键词生成

## 3、注意力机制

随机线索称之为查询（query），

每个输入是一个值（value）和不随意线索（key）的对，

通过注意力池化层来有偏向性地选择某些输入。





### 非参注意力池化层

- 给定数据$(x_i,y_i)$,i = 1,2,......n

- 平均池化是最简单的方案：$f(x) = \frac{1}{n}\sum_iy_i$

- 更好的方案是60年代提出的XX-XX核回归

  $f(x)=\frac{1}{n}\sum_{i=1}^n\frac{K(x-x_i)}{\sum_j^nK(x-x_j)}y_i$

### 参数化的注意力机制



### 1、普通模式

​		硬性注意力机制和软性注意力机制

#### （1）在所有输入信息上计算注意力分布

- ​	**把输入信息向量X看做是一个信息存储器，现在给定一个查询向量q，用来查找并选择X中的某些信息**。
- ​    $\alpha=softmax(s(x_i,q)) = \frac{e^{s(x_i,q)}}{\sum_{j=1}^{N}e^{s(x_j,q)}}$

#### （2）根据注意力分布来计算输入信息的加权平均

- ​	$att(X,q)=\sum_{i=1}^{N}\alpha_ix_i$

### 2、键值对注意力模式

​		可以用键值对（key-value pair）来表示输入信息，那么N个输入信息就可以表示为[(k1,v1),(k2,v2)......(kN,vN)]，其中“key”用来计算注意分布$\alpha_i$，“value”用来计算聚合信息。

​		通过计算key与query的相似度，就从对应的value中取出多少信息。每个地址Key对应的Value值都会被抽取内容出来，然后求和，这就相当于由Query与Key的相似性来计算每个Value值的权重，然后对Value值进行加权求和。加权求和得到最终的Value值，也就是Attention值。



### 3、Encoder-Decoder框架

​		**K是需要被翻译的语言经过Encoders之后的词向量表示。**

​		**Q是目标语言的所有单词的特征向量对应的矩阵（假设目标语言有5000个单词参加训练，那么矩阵Q就有5000行）**，

​		此时Q与$K^T$作点乘运算，可以计算出每个源单词翻译到目标单词对应的权重。最后再经过softmax和与目标单词的V矩阵作加权求和，经过一个分类器就能翻译出每一个源单词对应的目标单词。





### 4、多头注意力机制

​		多头注意力机制是利用多个查询变量Q=[q1,q2,q3,...,qm]，并行地从输入信息（K，V）= [(k1,v1), (k2,v2),...]中选取多组信息。在查询过程中，每个查询向量qi都将会关注输入信息的不同部分，即从不同的角度上去分析当前的输入信息。





### 5、自注意力机制

​		Q = K，**自注意力机制**（**self-Attention**）中，这里的**查询向量**也可以使用**输入信息**进行生成，而不是选择一个上述**任务相关**的**查询向量**。相当于模型读到**输入信息**后，根据**输入信息本身**决定当前最重要的信息。







### 数据集

NLPCC2017中文情感对话数据集

## ECM

Emotion category embedding  嵌入向量

情感向量：value decay 情感完全表达

情感强度：Memory字典的方式		type selector



评价方法：

#### 自动评价

perplexity：困惑度，**句子概率越大，语言模型越好，迷惑度越小**

情感准确度。事先指定一种情感类别，将生成回复放入情感分类器中得到回复的情感类别，统计两情感类别一致的比例，通常来衡量模型对情感的控制能力。

pairwise

#### 人工评价

在 连贯性 、逻辑性 、情感性上012打分。

综合评价：质量和不同性



### 情绪支持框架

三阶段对话框架：探索问题、理解和安抚、提出建议





## GAN总结

（1）生成器与判别器
1、生成器(Generator)：通过机器生成数据（这些内容可以是图片、文字，也可以是音乐），目的是“骗过”判别器
2、判别器(Discriminator)：判断这写内容是真实的还是机器生成的，目的是找出生成器做的“假数据”
（2）训练步骤
Step1: 固定判别器，训练生成器;

Step2: 固定生成器，训练判别器;

Step3：纳什均衡：*纳什均衡*是博弈论中一种解的概念，它是指满足下面性质的策略组合：任何一位玩家在此策略组合下单方面改变自己的策略（其他玩家策略不变）都不会提高自身的收益。

（3）样本图片生成过程
Step1：生成器输入随机噪声图A;

Step2：生成器卷积神经网络提取轮毂缺陷边缘特征，生成样本图；

Step3：判别器判别真实样本与生成样本，若辨别概率均为0.5，则输出样本，否之继续训练

Step4：样本库扩充。



